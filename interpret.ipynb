{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainable AI for Perses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll use [Captum](https://captum.ai/) from the Facebook AI team to inspect our perses model.\n",
    "\n",
    "Before we get going, make sure you have captum installed. See here for more information: https://captum.ai/#quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import GradientShap\n",
    "from captum.attr import Occlusion\n",
    "from captum.attr import NoiseTunnel\n",
    "from captum.attr import visualization as viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dnet_dataset.src.dnet_dataloader import DamageNetDataset\n",
    "from dnet import Net as Perses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matplotlib color scheme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_cmap = LinearSegmentedColormap.from_list('custom blue', \n",
    "                                                 [(0, '#ffffff'),\n",
    "                                                  (0.5, '#000000'),\n",
    "                                                  (1, '#000000')], N=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_DIR = 'data/images/'\n",
    "LABELS_DIR = 'data/labels/'\n",
    "PERSES_MODEL = 'model/perses.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((75, 75)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DamageNetDataset(images_dir=IMAGES_DIR, labels_dir=LABELS_DIR, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perses\n",
    "\n",
    "Load and initialize perses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perses = Perses()\n",
    "perses.load_state_dict(torch.load(PERSES_MODEL))\n",
    "perses.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainable AI\n",
    "\n",
    "In this section we'll define our explanation functions. These will help us interpret the model by visualizing important features of the image, that influenced the model most.\n",
    "\n",
    "The following functions are adapted from the [captum introductory tutorial](https://captum.ai/tutorials/Resnet_TorchVision_Interpret) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient-based approach\n",
    "\n",
    "Working back up the gradients, i.e. differentiating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_explain(img, largest_idx):\n",
    "    integrated_gradients = IntegratedGradients(perses)\n",
    "    attributions_ig = integrated_gradients.attribute(img, target=largest_idx, n_steps=200)\n",
    "    \n",
    "    noise_tunnel = NoiseTunnel(integrated_gradients)\n",
    "\n",
    "    attributions_ig_nt = noise_tunnel.attribute(img, n_samples=10, nt_type='smoothgrad_sq', target=largest_idx)\n",
    "    _ = viz.visualize_image_attr_multiple(np.transpose(attributions_ig_nt.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                                          np.transpose(img.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                                          [\"original_image\", \"heat_map\"],\n",
    "                                          [\"all\", \"positive\"],\n",
    "                                          cmap=default_cmap,\n",
    "                                          show_colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occlusion-based approach\n",
    "\n",
    "Cover parts of the image with a sliding window and see what changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occlusion_explain(img, largest_idx):\n",
    "    occlusion = Occlusion(perses)\n",
    "\n",
    "    attributions_occ = occlusion.attribute(img, strides = (3, 8, 8),\n",
    "                                       target=largest_idx,\n",
    "                                       sliding_window_shapes=(3,15, 15),\n",
    "                                       baselines=0)\n",
    "    \n",
    "    _ = viz.visualize_image_attr_multiple(np.transpose(attributions_occ.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                                          np.transpose(img.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                                          [\"original_image\", \"heat_map\"],\n",
    "                                          [\"all\", \"positive\"],\n",
    "                                          show_colorbar=True,\n",
    "                                          outlier_perc=2,\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Just hit enter when asked whether you want to continue.\n",
    "\n",
    "You may quit by interrupting the kernel under `Kernel` > `Interrupt kernel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, label in dataiter:    \n",
    "    output = perses(img)\n",
    "    output = torch.sigmoid(output).round()\n",
    "    \n",
    "    largest_idx = 0\n",
    "    for i, val in enumerate(output.to('cpu').tolist()[0]):\n",
    "        if val == 1:\n",
    "            largest_idx = i\n",
    "    \n",
    "    print('Target: ', label)\n",
    "    print('Ouput: ', output)\n",
    "    print(largest_idx)\n",
    "    \n",
    "    _ = input('Explain?')\n",
    "    \n",
    "    gradient_explain(img, largest_idx)\n",
    "    \n",
    "    _ = input('Next?')\n",
    "    \n",
    "    occlusion_explain(img, largest_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
